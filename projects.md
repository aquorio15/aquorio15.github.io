---
layout: page
title: "" 
---
<ul>
 <span style="background-color: bisque"><strong>Neural Machine Translation for Low-resource Tribal Language</strong></span>&nbsp;  
    <li>Worked on developing a POS tagger, lemmatizer, and stemmer for Lambani a low-resource language in India. The language technologies were then used to create an online dictionary to be used by the Tribal community.</li>
    <li>Collaborating with multiple NGOs around the country to develop Language Technologies for the marginalized communities and working on solutions to deploy them in resource-intensive scenarios.</li>
    <li>Development of resource-efficient Machine Translation (MT) model which can be deployed on mobile devices.</li>
</ul>

<ul>
 <span style="background-color: bisque"><strong>Development of multimodal LLM</strong></span>&nbsp;
    <li>Spearheaded the development of a multi-modal LLM system, integrating vision, speech, and text capabilities, resulting
    in 40% improvement in cross-domain task performance.</li>
    <li>Implemented a scalable LLM fine-tuning pipeline, reducing model adaptation time by 75% and enabling rapid deployment of domain-specific models.</li>
    <li>Contributed to the development of a novel few-shot learning algorithm, enabling LLMs to perform complex reasoning tasks with 70% fewer examples.</li>
</ul>

<ul>
 <span style="background-color: bisque"><strong>Model optimization in LLMs</strong></span>&nbsp;
    <li>Orchestrated the development of a novel compression reducing model size by 70% while maintaining 95% of original performance technique for a multimodal LLM.</li>
    <li>Experimenting with various parameter-efficient finetuning methods (LoRA, QLoRA) and reducing the memory demand of LLMs by efficient “KV” cache compression.</li>
    <li>Worked on mitigating Hallucinations in LLMs by using the model itself to do verification and revision, without any use of external knowledge.</li>
</ul>

<ul>
  <span style="background-color: bisque"><strong>Text-Visual language understanding</strong></span>&nbsp;
    <li>Developed a multilingual VQA dataset that contains spoken questions in multiple languages.</li>
    <li>Led a cross-functional team of 3 engineers in designing and deploying an text-to-image model.</li>
    <li>Implemented a novel state-space architecture for diffusion models for image generation task.</li>
</ul>

<ul>
 <span style="background-color: bisque"><strong>Memorization problem in LLM</strong></span>&nbsp;
    <li>Explored the memorization problem in LLMs due to pretraining on large amount of data, and ways in which we can delete information without using conventional finetuning methods.</li>
    <li>Explored various model quantization techniques.</li>
    <li>Developed a bandit based method to selectively delete patches in a pretrained ViT (Vision Transformer).</li>
</ul>
